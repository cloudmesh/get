#!/usr/bin/env python
#
# PLEASE DO NOT EDIT IT DIRECTLY ON cloudmesh.github.io
#
# Instead, if modifications are needed, modify it here  
#
# * https://github.com/cloudmesh/get/blob/main/pi/slurm/index.html
#
#   curl -Ls https://raw.githubusercontent.com/cloudmesh/get/main/pi/slurm/index.html | python
#
# <pre>

from cloudmesh.common.StopWatch import StopWatch
from cloudmesh.common.util import banner
from cloudmesh.common.parameter import Parameter
from cloudmesh.common.Host import Host
from cloudmesh.common.util import readfile,writefile
from cloudmesh.common.Printer import Printer
from cloudmesh.common.console import Console
from cloudmesh.common.util import yn_choice
from cloudmesh.common.Shell import Shell
from pprint import pprint
import os
import sys
import re
import textwrap
import time
import subprocess
from cloudmesh.burn.usb import USB
from cloudmesh.burn.sdcard import SDCard

#hosts = "red,red0[1-3]"
#workers = "red0[1-3]"
#manager = "red"

## Defining repeat attempt function

def hline(c="="):
    print(79*c)

def execute(commands, sleep_time=1, repeat=10, driver=Shell.run):
    hline()
    print(commands)
    hline()

    result = ""
    commands = textwrap.dedent(commands).strip()
    c=0
    for command in commands.splitlines():

        #c = c + 1
        #print(c,command)
        #add_history(command)

        if command.strip().startswith("#"):
            print(command)

        else:
            success = False
            counter = 0
            while not success:
                banner(f"{command}")
                sys.stdout.flush()
                r = driver(command)
                sys.stdout.flush()
                hline(c="-")
                if r:
                    print("error")
                else:
                    success=True
                #print(r)
                time.sleep(sleep_time)
                counter = counter + 1
                if repeat < 0:
                    pass
                else:

                    success = success or counter >= repeat
            #os.system("sync")
            if counter == repeat:
                raise RuntimeError(f"Maximum repeats reached: {command}")


    #return result


def hostexecute(script, manager):
    for command in script.splitlines():
        print(command)
        results = Host.ssh(hosts=manager, command=command)
        print(Printer.write(results))


def managerNamer():
    manager = subprocess.run(['hostname'], capture_output=True, text=True).stdout.strip()
    print(f"The hostname of the manager is taken to be {manager} \n")
    return manager


# defining function which formulates hosts variable
# the hosts variable has manager and workers

def hostsVariable(manager,workers):
    hosts = f'''{manager},{workers}'''
    hosts = str(hosts)
    return(hosts)

# read the file user_input_workers
def read_user_input_workers(manager):
    results = Host.ssh(hosts=manager, command='cat user_input_workers')
    print(Printer.write(results))
    for entry in results:
        print(str(entry["stdout"]))
        workers = str(entry["stdout"])
        return workers

# tell user to ssh back to manager on reboot and reboot
def tell_user_rebooting():
    banner('''The cluster is rebooting. Wait a minute for the Pis to come back online and ssh into the manager. Then,'''
           ''' rerun the script by issuing "python3 slurm.py" to continue.''')
    os.system("cms host reboot "+hosts)

# function that returns ip of pi
def get_IP(manager):
    results = Host.ssh(hosts=manager, command="/sbin/ifconfig eth0 | grep 'inet' | cut -d: -f2")
    print(Printer.write(results))
    for entry in results:
        print(str(entry["stdout"]))
        ipaddress = str(entry["stdout"])
    print(ipaddress)
    ipaddress2 = ipaddress.replace('inet ', '')
    print(ipaddress2)
    ipaddress3 = ipaddress2.split(' ')
    print(ipaddress3)
    ipaddress4 = [x for x in ipaddress3 if x]
    print(ipaddress4)
    trueIP = ipaddress4[0]
    print(trueIP)
    return trueIP

# Beginning to define SLURM installation

# input
#  basename = "red"
#  no workers = "03" # "3" "01000" 00001-01000
# output
#  red000  -> red, red001, red002, red003

def step0():
    StopWatch.start("Current section time")
    banner("Welcome to SLURM Installation. Initializing preliminary steps.")
    print("We assume that you run this script on the manager Pi and that your worker naming schema is \n"
          "incremental in nature. \n")
    manager = managerNamer()
    user_input_workers = input(str('''Please enter the naming schema of your workers. For example, if you have 3 
        workers then enter "red0[1-3]". Another example for 7 workers is "worker[1-7]" (do not include
        quotation marks): \n'''))

    results = Host.ssh(hosts=manager, command="touch user_input_workers")
    print(Printer.write(results))
    results = Host.ssh(hosts=manager, command=f'''echo '{user_input_workers}' >> user_input_workers''')
    print(Printer.write(results))

    # intro and asking for workers from user
    workers = read_user_input_workers(manager)

    hosts = hostsVariable(manager,workers)

    results = Host.ssh(hosts=hosts, command="touch step0")
    print(Printer.write(results))
    StopWatch.stop("Current section time")
    StopWatch.benchmark()

def step1(results):
    StopWatch.start("Current section time")
    # intro and asking for workers from user
    banner("Initializing Step 1 now.")
    manager = managerNamer()
    print(Printer.write(results))

    workers = read_user_input_workers(manager)

    hosts = hostsVariable(manager,workers)

    banner("Now updating packages. This may take a while.")
    results = Host.ssh(hosts=hosts, command="sudo apt-get update")
    print(Printer.write(results))
    #parallel_execute(hosts,"sudo apt install ntpdate -y")
    #results2 = Host.ssh(hosts=hosts, command="sudo apt install ntpdate -y")
    #print(Printer.write(results2))

    # make array with list of workers
    listOfWorkers = Parameter.expand(workers)

    print(listOfWorkers)
    for worker in listOfWorkers:
        print(f"Now updating ntpdate module for {worker}.")
        success = False
        while not success:
            success = True
            results = Host.ssh(hosts=[worker],
                               command="sudo apt install ntpdate -y")
            print(Printer.write(results))
            for entry in results:
                if 'Could not connect to' in str(entry["stdout"]):
                    msg = "The script had trouble installing packages. Retrying."
                    banner(msg)
                    # return msg
                    success = False
            time.sleep(10)
    results = Host.ssh(hosts=hosts, command="touch step1")
    print(Printer.write(results))
    StopWatch.stop("Current section time")
    StopWatch.benchmark()
    tell_user_rebooting()


def step2():
    StopWatch.start("Current section time")
    banner("Initializing Step 2 now.")
    manager = managerNamer()
    if not yn_choice('Please insert USB storage medium into top USB 3.0 (blue) port on manager pi and press y when done'):
        Console.error("You pressed no but the script is continuing as normal...")
        return ""
    '''
    os.system('lsblk')
    if not yn_choice('Please confirm that sda1 is your USB WHICH WILL BE FORMATTED by pressing y'):
        Console.error("Terminating: User Break")
        return ""
        sys.exit()
    '''

    # executing reading of workers
    workers = read_user_input_workers(manager)

    hosts = hostsVariable(manager,workers)

    card = SDCard()
    card.info()
    USB.check_for_readers()
    print('Please enter the device path e.g. "/dev/sda" or no input default to /dev/sda (remember, do not add '
          'quotation marks)')
    print('The device of the path you enter WILL BE FORMATTED and used as cluster file storage for SLURM config:')
    device = input()
    if device == '':
        device = '/dev/sda'
    print(device)
    script = textwrap.dedent(
        f"""
        sudo mkfs.ext4 -F {device}
        sudo mkdir /clusterfs
        sudo chown nobody.nogroup -R /clusterfs
        sudo chmod 777 -R /clusterfs
        """).strip()
    hostexecute(script, manager)

    #results = Host.ssh(hosts=manager, command=f"sudo mkfs.ext4 -F {device}")
    #print(Printer.write(results))
    #results = Host.ssh(hosts=manager, command="sudo mkdir /clusterfs")
    #print(Printer.write(results))
    #results = Host.ssh(hosts=manager, command="sudo chown nobody.nogroup -R /clusterfs")
    #print(Printer.write(results))
    #results = Host.ssh(hosts=manager, command="sudo chmod 777 -R /clusterfs")
    #print(Printer.write(results))

    results = Host.ssh(hosts=manager, command=f"sudo blkid {device}")
    print(Printer.write(results))
    for entry in results:
        print(str(entry["stdout"]))
        blkid = str(entry["stdout"])
    print(blkid)
    blkid2 = re.findall(r'\S+', blkid)
    print(blkid2)
    result = [i for i in blkid2 if i.startswith('UUID=')]
    print(result)
    listToStr = ' '.join(map(str, result))
    result2 = re.findall(r'"([^"]*)"', listToStr)
    result2 = " ".join(str(x) for x in result2)
    print(type(result2))
    print(result2)
    script = textwrap.dedent(
        f"""
        echo "UUID={result2} /clusterfs ext4 defaults 0 2" | sudo tee /etc/fstab -a
        sudo mount -a
        sudo chown nobody.nogroup -R /clusterfs
        sudo chmod -R 766 /clusterfs
        sudo apt install nfs-kernel-server -y
        """)
    hostexecute(script, manager)
    trueIP = get_IP(manager)
    results = Host.ssh(hosts=manager, command=f'''sudo cat /etc/exports''')
    print(Printer.write(results))
    for entry in results:
        Preexisting = False
        if f'/clusterfs {trueIP}/24(rw,sync,no_root_squash,no_subtree_check)' in str(entry["stdout"]):
            Preexisting = True
    if not Preexisting:
        results = Host.ssh(hosts=manager, command=f'''echo "/clusterfs {trueIP}/24(rw,sync,no_root_squash,no_subtree_check)" | sudo tee /etc/exports -a''')
        print(Printer.write(results))

    hostexecute("sudo exportfs -a", manager)

    # make array with list of workers
    listOfWorkers = Parameter.expand(workers)

    print(listOfWorkers)
    for worker in listOfWorkers:
        print(worker)
        success = False
        while not success:
            success = True
            results = Host.ssh(hosts=[worker],
                               command="sudo apt install nfs-common -y")
            print(Printer.write(results))
            for entry in results:
                if 'Could not connect to' in str(entry["stdout"]):
                    msg = "The script had trouble installing packages. Retrying."
                    banner(msg)
                    # return msg
                    success = False
            time.sleep(10)
    results = Host.ssh(hosts=workers, command='sudo mkdir /clusterfs')
    print(Printer.write(results))
    results = Host.ssh(hosts=workers, command='sudo chown nobody.nogroup /clusterfs')
    print(Printer.write(results))
    results = Host.ssh(hosts=workers, command='sudo chmod -R 777 /clusterfs')
    print(Printer.write(results))

    results = Host.ssh(hosts=manager, command=f'''sudo cat /etc/fstab''')
    print(Printer.write(results))
    for entry in results:
        Preexisting = False
        if f'{trueIP}:/clusterfs    /clusterfs    nfs    defaults   0 0' in str(entry["stdout"]):
            Preexisting = True
    if not Preexisting:
        results = Host.ssh(hosts=workers, command=f'''echo "{trueIP}:/clusterfs    /clusterfs    nfs    defaults   0 0" | sudo tee /etc/fstab -a''')
        print(Printer.write(results))
    results = Host.ssh(hosts=hosts, command="touch step2")
    print(Printer.write(results))
    StopWatch.stop("Current section time")
    StopWatch.benchmark()
    tell_user_rebooting()


def step3():
    StopWatch.start("Current section time")
    banner("Initializing Step 3 now.")

    manager = managerNamer()
    # getting ip in case step 2 has not run
    trueIP = get_IP(manager)

    # executing reading of workers
    workers = read_user_input_workers(manager)

    hosts = hostsVariable(manager,workers)

    #now doing actual step 3
    script = textwrap.dedent(
        f"""
        sudo systemctl status nfs-server.service
        sudo systemctl start nfs-server.service
        sudo mount -a
        sudo apt install slurm-wlm -y
        sudo cp /usr/share/doc/slurm-client/examples/slurm.conf.simple.gz /etc/slurm-llnl
        sudo gzip -d /etc/slurm-llnl/slurm.conf.simple.gz
        sudo mv /etc/slurm-llnl/slurm.conf.simple /etc/slurm-llnl/slurm.conf
        sudo sed -i 's/SlurmctldHost=workstation/SlurmctldHost={manager}({trueIP})/g' /etc/slurm-llnl/slurm.conf
        sudo sed -i "$(( $(wc -l </etc/slurm-llnl/slurm.conf)-2+1 )),$ d" /etc/slurm-llnl/slurm.conf
        """)
    hostexecute(script, manager)
    results = Host.ssh(hosts=workers, command="cat /proc/sys/kernel/hostname")
    print(Printer.write(results))
    hostnames = []
    for entry in results:
        currentHostname = str(entry["stdout"])
        hostnames.append(currentHostname)
    print(hostnames)
    results = Host.ssh(hosts=workers, command="/sbin/ifconfig eth0 | grep 'inet' | cut -d: -f2")
    ipaddresses = []
    trueIPs = []
    print(Printer.write(results))
    for entry in results:
        currentIP = str(entry["stdout"])
        ipaddresses.append(currentIP)
    for x in ipaddresses:
        x2 = x.replace('inet ', '')
        x3 = x2.split(' ')
        print(x3)
        x4 = [y for y in x3 if y]
        print(x4)
        trueIP = x4[0]
        print(trueIP)
        trueIPs.append(trueIP)
    print(trueIPs)
    coreCounts = []
    results = Host.ssh(hosts=workers, command="cat /sys/devices/system/cpu/cpu[0-9]*/topology/core_cpus_list | sort -u | wc -l")
    for entry in results:
        currentCoreCount = str(entry["stdout"])
        coreCounts.append(currentCoreCount)
    for x in range(len(hostnames)):
        results = Host.ssh(hosts=manager,
                           command=f'''echo "NodeName={hostnames[x]} NodeAddr={trueIPs[x]} CPUs={coreCounts[x]} State=UNKNOWN" | sudo tee /etc/slurm-llnl/slurm.conf -a''')
        print(Printer.write(results))

    script = textwrap.dedent(
        f"""
        echo "PartitionName=mycluster Nodes={workers} Default=YES MaxTime=INFINITE State=UP" | sudo tee /etc/slurm-llnl/slurm.conf -a
        sudo curl -L https://github.com/cloudmesh/cloudmesh-mpi/raw/main/doc/chapters/slurm/cgroup.conf > ~/cgroup.conf
        sudo curl -L https://github.com/cloudmesh/cloudmesh-mpi/raw/main/doc/chapters/slurm/cgroup_allowed_devices_file.conf > ~/cgroup_allowed_devices_file.conf
        sudo cp ~/cgroup.conf /etc/slurm-llnl/cgroup.conf
        sudo cp ~/cgroup_allowed_devices_file.conf /etc/slurm-llnl/cgroup_allowed_devices_file.conf
        sudo rm ~/cgroup.conf
        sudo rm ~/cgroup_allowed_devices_file.conf
        sudo cp /etc/slurm-llnl/slurm.conf /etc/slurm-llnl/cgroup.conf /etc/slurm-llnl/cgroup_allowed_devices_file.conf /clusterfs
        sudo cp /etc/munge/munge.key /clusterfs
        sudo systemctl enable munge
        sudo systemctl start munge
        sudo systemctl enable slurmctld
        sudo systemctl start slurmctld
        """)
    hostexecute(script, manager)        

    results = Host.ssh(hosts=hosts, command="touch step3")
    print(Printer.write(results))
    StopWatch.stop("Current section time")
    StopWatch.benchmark()
    tell_user_rebooting()


def step4():
    StopWatch.start("Current section time")
    banner("Initializing Step 4 now.")
    manager = managerNamer()
    # executing reading of workers
    workers = read_user_input_workers(manager)

    hosts = hostsVariable(manager,workers)

    #getting hostname count
    results = Host.ssh(hosts=workers, command="cat /proc/sys/kernel/hostname")
    print(Printer.write(results))
    hostnames = []
    for entry in results:
        currentHostname = str(entry["stdout"])
        hostnames.append(currentHostname)
    print(hostnames)
    '''
    script = textwrap.dedent(
        f"""
        sudo chown nobody.nogroup /clusterfs
        sudo chmod -R 777 /clusterfs
        #
        # sudo bash -c "echo 'nameserver 8.8.8.8 8.8.4.4' >> /etc/resolv.conf"
        # sudo bash -c "> /etc/apt/sources.list"
        # sudo bash -c "echo 'deb http://raspbian.mirrors.wvstateu.edu/raspbian/ buster main contrib non-free rpi' >> /etc/apt/sources.list"
        #
        sudo apt-get update -y
        sudo apt update -y
        sudo apt full-upgrade -y
        """)
    hostexecute(script, workers)
    '''

    listOfWorkers = Parameter.expand(workers)

    results = Host.ssh(hosts=workers, command='sudo chown nobody.nogroup /clusterfs')
    print(Printer.write(results))
    results = Host.ssh(hosts=workers, command='sudo chmod -R 777 /clusterfs')
    print(Printer.write(results))
    results = Host.ssh(hosts=workers, command='sudo apt-get update -y')
    print(Printer.write(results))
    results = Host.ssh(hosts=workers, command='sudo apt update -y')
    print(Printer.write(results))
    results = Host.ssh(hosts=workers, command='sudo apt full-upgrade -y')
    print(Printer.write(results))

    #
    #  HERE IS A POTENTIAL ISSUE AS THE NEXT COMMAND SEEMS TO FAIL OFTEN
    #

    # PPOS ERROR HERE
    #
    #
    # Err:3 http://raspbian.mirror.constant.com/raspbian buster/main armhf fonts-dejavu-core all 2.37-1
    # Could not connect to raspbian.mirror.constant.com:80 (2001:19f0:1595:1403::1087),
    # connection timed out Could not connect to raspbian.mirror.constant.com:80 (108.61.5.87), connection timed out


    #
    # this is no longer executed in parallel, but for each worker one by one. We coudl use a process pool to avoid
    # restrictions on the mirror that may limit parallel connections
    #
    for worker in listOfWorkers:
        success = False
        while not success:
            success = True
            results = Host.ssh(hosts=worker,
                               command="sudo apt install slurmd slurm-client -y --fix-missing")
            print(Printer.write(results))
            for entry in results:
                if 'Could not connect to' in str(entry["stdout"]):
                    msg = f"The SLURM script could not install needed packages, but will try again. " \
                          f"This is expected behavior and it should fix itself within a few minutes. " \
                          f"Currently fixing {worker}."
                    banner(msg)
                    # return msg
                    success = False
                else:
                    banner(f"Package installation has succeeded for {worker}. Moving on to next device.")
            time.sleep(5)
    #
    #  HERE IS END OF THAT POTENTIAL ISSUE
    #

    '''
    script = textwrap.dedent(
        f"""
        sudo cp /clusterfs/munge.key /etc/munge/munge.key
        sudo cp /clusterfs/slurm.conf /etc/slurm-llnl/slurm.conf
        sudo cp /clusterfs/cgroup* /etc/slurm-llnl
        sudo systemctl enable munge
        sudo systemctl start munge
        sudo systemctl enable slurmd
        sudo systemctl start slurmd
        """)
    hostexecute(script, workers)
    '''
    results = Host.ssh(hosts=workers, command='sudo cp /clusterfs/munge.key /etc/munge/munge.key')
    print(Printer.write(results))
    results = Host.ssh(hosts=workers, command='sudo cp /clusterfs/slurm.conf /etc/slurm-llnl/slurm.conf')
    print(Printer.write(results))
    results = Host.ssh(hosts=workers, command='sudo cp /clusterfs/cgroup* /etc/slurm-llnl')
    print(Printer.write(results))
    results = Host.ssh(hosts=workers, command='sudo systemctl enable munge')
    print(Printer.write(results))
    results = Host.ssh(hosts=workers, command='sudo systemctl start munge')
    print(Printer.write(results))
    results = Host.ssh(hosts=workers, command='sudo systemctl enable slurmd')
    print(Printer.write(results))
    results = Host.ssh(hosts=workers, command='sudo systemctl start slurmd')
    print(Printer.write(results))
    
    script = textwrap.dedent(
        f"""
        sinfo
        sudo sed -i 's/ReturnToService=1/ReturnToService=2/' /etc/slurm-llnl/slurm.conf
        """)
    hostexecute(script, manager)

    StopWatch.stop("Current section time")
    StopWatch.benchmark()
    banner("SLURM is now installed.")
    results = Host.ssh(hosts=hosts, command="touch step4")
    print(Printer.write(results))
    print("Rebooting cluster now.")
    banner("After successful reboot, ssh back into manager and test SLURM by issuing $ srun --nodes=3 hostname "
           "(change 3 to number of nodes if necessary). If it does not work right away, wait a minute for the "
           "nodes to come back online.\n")
    os.system("cms host reboot "+hosts)


#a = readfile("test1")
#writefile("test2",a)

#StopWatch.start("Total Runtime")

banner("SLURM on Raspberry Pi Cluster Installation")
manager = managerNamer()
results9001 = Host.ssh(hosts=manager, command="ls step0")
print(Printer.write(results9001))
step0done = True
for entry in results9001:
    if 'step0' in str(entry["stdout"]) and 'cannot access' in str(entry["stdout"]):
        step0done = False
        entry["stderr"] = "False"

if not step0done:
    step0()

# executing reading of workers
workers = read_user_input_workers(manager)

hosts = hostsVariable(manager,workers)

results = Host.ssh(hosts=hosts,command="ls step1")

completed = True
for entry in results:
    if 'step1' in str(entry["stderr"]) and 'cannot access' in str(entry["stderr"]):
        completed = False
        entry["stderr"] = "False"
if completed:
    banner("Step 1 is done.")
    pprint(results)
    results42 = Host.ssh(hosts=hosts, command="ls step2")
    completedSecondStep = True
    for entry in results42:
        if 'step2' in str(entry["stderr"]) and 'cannot access' in str(entry["stderr"]):
            completedSecondStep = False
            entry["stderr"] = "False"
    if completedSecondStep:
        banner("Step 2 is done.")
    if not completedSecondStep:
        banner("Step 2 is not done. Performing Step 2 now.")
        step2()
if not completed:
    banner("Step 1 is not done. Performing step 1 now.")
    step1(results)
    #os.system('touch step1')


results42 = Host.ssh(hosts=hosts,command="ls step2")
completed2 = True
for entry in results42:
    if 'step2' in str(entry["stderr"]) and 'cannot access' in str(entry["stderr"]):
        completed2 = False
        entry["stderr"]="False"
if completed2:
    banner("Step 2 is done.")
    pprint(results42)
    results43 = Host.ssh(hosts=hosts, command="ls step3")
    completedThirdStep = True
    for entry in results43:
        if 'step3' in str(entry["stderr"]) and 'cannot access' in str(entry["stderr"]):
            completedThirdStep = False
            entry["stderr"] = "False"
    if not completedThirdStep:
        banner("Step 3 is not done. Performing step 3")
        step3()
if not completed2:
    banner("Step 2 is not done. Performing step 2 now.")
    step2()
    #os.system('touch step2')

results50 = Host.ssh(hosts=hosts,command="ls step3")
completedThirdStep = True
for entry in results50:
    if 'step3' in str(entry["stderr"]) and 'cannot access' in str(entry["stderr"]):
        completedThirdStep = False
        entry["stderr"]="False"
if completedThirdStep:
    banner("Step 3 is done.")
if not completedThirdStep:
    banner("Step 3 is not done. Performing step 3 now.")
    step3()

results51 = Host.ssh(hosts=hosts,command="ls step4")
completedFourthStep = True
for entry in results51:
    if 'step4' in str(entry["stderr"]) and 'cannot access' in str(entry["stderr"]):
        completedFourthStep = False
        entry["stderr"]="False"
if completedFourthStep:
    banner("Step 4 is done so SLURM should already be installed.")
if not completedFourthStep:
    banner("Step 4 is not done. Executing step 4 now.")
    step4()



'''
#def parallel_execute(hosts,command):
#  os.system("cms host ssh "+hosts+" \"'"+command+""'\"")
#
#parallel_execute(hosts,"sudo apt-get update")
#for host in ehosts:
#  os.system("cms host ssh "+hosts+" 'touch step1'")
# print (names)
results4 = Host.ssh(hosts=hosts,
                   command="cat step1")
print(results4)
StopWatch.stop("Total Runtime")
StopWatch.benchmark()
'''
